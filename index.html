<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description">
<!--        content="DynamicImportance: Consistent, Dynamic, and Extendable Long Video Generation from Text.">
  <meta name="keywords" content="DynamicImportance">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>DynamicImportance: Interactively Assigning Word Importance to a Generalist Vision Model</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <style>
    .inline-equation {
      display: inline-block;
      vertical-align: middle;
    }
  </style>

  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.png">

  <style>  
    table {  
      font-family: arial, sans-serif;  
      border-collapse: collapse;  
      width: 100%;  
    }  
      
    td, th {  
      border: 2px solid #F1F4F5;  
      text-align: left;  
      padding: 8px;  
    }  
    
    tr:nth-child(3n - 1) {  
      background-color: #F1F4F5;  
    }  

    tr:nth-child(3n) {  
      border: 2px solid #FFFFFF;
    }  
  </style>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"> DynamicImportance: <br> Interactive Assignment of Word Significance in a Generalist Vision Model </h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://www.linkedin.com/in/meri-topuzyan-606047191/">Meri Topuzyan</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://levon-kh.github.io/">Levon Khachatryan</a><sup>1,2</sup>
            </span>
            <br>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Yerevan State University (YSU),</span>
            <span class="author-block"><sup>2</sup>Picsart AI Research (PAIR),</span>
            <span class="author-block"><sup>3</sup>Cognaize Engineering</span>
          </div>
          <div class="column has-text-centered">
            <div class="publication-links">

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img id="teaser" autoplay muted loop playsinline height="100%" src="./static/images/teaser.jpg" style="width:100%;height:100%;">
      <p class="subtitle has-text-centered">
        <b><span class="dnerf">DynamicImportance</span></b>  introduces a transformative method to refine how textual prompts influence image synthesis in vision-based AI models, specifically within the InstructDiffusion framework. This novel approach dynamically adjusts the importance assigned to specific words within textual prompts, directly impacting the attention mechanisms of the model. This results in more accurate and contextually relevant image outputs that closely align with human instructions, thereby advancing the model's utility in practical and research applications.
      </p>
    </div>
  </div>
</section>


<section class="section b-section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The <span class="dnerf">Recent advancements in Deep Learning have markedly expanded the potential for creating a generic model for various vision tasks. At the forefront, <span class="dnerf">InstructDiffusion</span> presented a unified framework for aligning various computer vision tasks with human instructions (textual prompts). The model can handle a variety of vision tasks, including understanding tasks (such as segmentation and keypoint detection) and generative tasks (such as editing and enhancement). It is built upon the diffusion process and is trained to predict pixels according to user instructions, such as "encircling the man's left shoulder in red." (keypoint detection) or "applying a blue mask to the left car." (segmentation). However, not all words in the given textual prompt hold equal importance. Certain indicator words (e.g. "left shoulder" in the previous example) require more emphasis than others. To address this limitation, we propose DynamicImportance, a post-hoc method inspired by <span class="dnerf">Attend-and-Excite</span>. This technique conducts On-the-Fly optimization to direct the model in refining the cross-attention units based on the specified weights assigned to each word. The mechanism selectively enhances relevant features and suppresses less informative ones, thereby refining the model's focus during the image editing process. By dynamically adjusting to the content specified in text instructions, it allows for more nuanced and context-aware modifications of images. This strategic enhancement in attention processing not only improves the model's interpretability of textual instructions but also significantly boosts the precision and coherence of the resultant image edits. Our novel attention-augmented InstructDiffusion model, therefore, represents a symbiosis of interpretative and generative capabilities, paving the way for more intuitive and human-like editing performances in AI-driven systems.
          </p>
        </div>
      </div>
    </div>

  </div>
</section>


<section class="section b-section" id="Method-Detailed">
  <div class="container is-max-desktop content">
    <h2 class="title">Method</h2>
    <section class="hero method">
      <div class="container is-max-desktop">
        <div class="hero-body">
          <p>
              Attention Mechanism Enhancement: <span class="dnerf">DynamicImportance</span>  enhances the model’s attention mechanisms by implementing Attend-and-Excite strategies that focus on real-time adjustments based on the weighted significance of words in the textual prompt.
<!--          <p>-->
<!--          <p>-->
<!--            Algorithmic Implementation: The method operates by integrating an additional layer within the InstructDiffusion’s architecture that recalibrates attention distributions dynamically, aligning more closely with the intent articulated in the text prompts.-->
          </p>
          <img id="method" autoplay muted loop playsinline height="100%" src="./static/images/diagram.jpg" style="width:100%;height:80%;">
            <p>
              Text Parsing and Weight Assignment: The model parses input text and assigns dynamic weights to different words based on their semantic importance to the task.
            <p>
              Attention Modulation: Based on these weights, the model's attention mechanisms are adjusted to enhance the representation of critical features in the image synthesis process.
            </p>
              Image Synthesis: The adjusted attention weights influence the diffusion process, ensuring that the generated images more accurately reflect the detailed nuances of the input text.
            </p>

        </div>
      </div>
    </section>
  </div>
</section>

<section class="section" id="Key Features">
  <div class="container is-max-desktop content">
    <h2 class="title">Key Features</h2>
    <section class="hero method">
      <div class="container is-max-desktop">
        <div class="hero-body">
          <ul>
            <li>
              <strong>Interactive Word Significance Assignment</strong>
              <p>Users can actively influence the model's attention by specifying the importance of terms within the textual prompt. This interactive feature ensures that the model aligns closely with user intentions, enhancing the customization and relevance of the image generation process. The integration of your modifications has significantly reduced any previous inconsistencies in how the model prioritized text, ensuring a much more precise adherence to user-defined importance.</p>
            </li>
            <li>
              <strong>Real-Time Attention Adjustment</strong>
              <p>The model dynamically adjusts its focus on image areas that are directly relevant to the textual cues provided. This capability significantly enhances the specificity and accuracy of the generated images, ensuring that they are both high-quality and aligned with the input text. With the improvements implemented, the model now exhibits superior performance in maintaining focus on the intended areas without any localization errors, thereby improving the reliability and applicability of the generated images.</p>
            </li>
            <li>
              <strong>Context-Aware Image Synthesis</strong>
              <p>This feature enables the model to consider contextual information from the image itself, such as scene layout and existing objects. By understanding the context in which textual cues are placed, the model can make more informed decisions, leading to more coherent and contextually appropriate outputs. The enhancements you've added allow the model to better interpret and integrate contextual cues, virtually eliminating past issues related to out-of-context synthesis and enhancing the overall fidelity and applicability of the edits.</p>
            </li>
          </ul>

        </div>
      </div>
    </section>
  </div>
</section>


<section class="section b-section" id="results-Detailed">
  <div class="container is-max-desktop content">
    <h2 class="title">Results</h2>
    <section class="hero results">
      <div class="container is-max-desktop">
        <div class="hero-body">
          <p>

Evaluation of the performance of the InstructDiffusion and DynamicImportance models across various computer vision tasks designed to assess each model’s ability in object addition, color changes, object detection, segmentation, and artistic style applications. All figures referenced from left to right depict the original image, followed by the edits made by InstructDiffusion, and concluding with those made by DynamicImportance. Our analysis is centered on how precisely and effectively each model executes the given instructions.<!--          <p>-->
          </p>
          <img id="results_1" autoplay muted loop playsinline height="100%" src="static/images/example_1.jpg" style="width:100%;height:80%;">
          <img id="results_2" autoplay muted loop playsinline height="100%" src="static/images/example_2.jpg" style="width:100%;height:80%;">
          <img id="results_3" autoplay muted loop playsinline height="100%" src="static/images/example_3.jpg" style="width:100%;height:80%;">



        </div>
      </div>
    </section>
  </div>
</section>



<section class="section" id="Conclusion">
  <div class="container is-max-desktop content">
    <h2 class="title">Conclusion</h2>
    <section class="hero method">
      <div class="container is-max-desktop">
        <div class="hero-body">
          <p class="equalized-text">
This research has successfully introduced DynamicImportance, a methodical enhancement for the InstructDiffusion model which dynamically assigns importance to the words within textual prompts across a spectrum of computer vision tasks. This approach refines the cross-attention mechanisms within the model, ensuring that critical textual elements are emphasized. The effec- tiveness of DynamicImportance in enhancing the precision and contextual relevance of output im- ages represents a significant advance in text-driven image editing technologies. Our methodology builds on the foundational concepts of Attend-and-Excite by optimizing attention distributions to selectively amplify features that are crucial for a given task, while suppressing irrelevant informa- tion. This selective attention mechanism is crucial for tasks that require high fidelity and nuanced interpretation of textual instructions to generate visually coherent images. With DynamicImpor- tance we have demonstrated that it is possible to significantly boost the model’s performance, mak- ing it not only more accurate but also more responsive to the subtleties of human language. The experimental results underscore the robustness of our approach. Through those we have shown that DynamicImportance improves the model’s ability to interpret complex instructions, leading to outputs that are not only more accurate but also artistically and contextually aligned with the input prompts. This enhancement in performance is particularly evident in the model’s ability to handle intricate image manipulation tasks that traditional diffusion models often struggle with. Furthermore, this research has explored the implications of enhanced attention mechanisms in AI- driven systems, suggesting that similar approaches could be beneficial across various applications in computer vision and beyond. The ability of DynamicImportance to refine model outputs based on textual analysis could be extended to other areas such as automated content generation, real-time video editing, and other interactive tools, where precision and context sensitivity are paramount.
          </p>
        </div>
      </div>
    </section>
  </div>
</section>


<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/dynamicimportance.pdf" width="100%" height="550">
          </iframe>

      </div>
    </div>
  </section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <p> If you use our work in your research, please cite our publication: </p>
    <pre><code>@article{DynamicImportance,
    title={DynamicImportance: Interactive Assignment of Word Significance in a Generalist Vision Model},
    author={Topuzyan, Meri and Khachatryan, Levon},
    year={2024}
  }
    </code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <!-- FIX THE LINK -->
      <a class="icon-link" href="https://arxiv.org/abs/2403.14773">
        <i class="fas fa-file-pdf"></i>
      </a>
      <!-- <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled> -->
      <a class="icon-link" href="https://github.com/meritop98/DynamicImportance.github.io" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Website adapted from the following <a href="https://github.com/nerfies/nerfies.github.io">template</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
